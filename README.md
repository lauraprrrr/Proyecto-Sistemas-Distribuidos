# .ñ•î.„Éª„ÄÇ.„Éª„Çú‚ú≠„Éª.„Éª‚ú´„Éª„Çú„Éª„ÄÇ. Proyecto semestral Sistemas Distribuidos .„Éª„ÄÇ.„Éª„Çú‚ú≠„Éª.„Éª‚ú´„Éª„Çú„Éª„ÄÇ.

Este proyecto corresponde a la entrega final del curso de Sistemas Distribuidos 2025-1. Su objetivo es procesar de manera distribuida los eventos de tr√°fico recolectados desde el mapa de Waze (https://www.waze.com/es-419/live-map/). Las alertas de trafico son extraidas, almacenadas, filtradas y finalmente pueden ser visualizadas con Kibana de Elasticsearch. Adem√°s contiene un m√≥dulo cache que funciona con simulaci√≥n de tr√°fico generado artificialmente(modulo generator), esto con el objetivo de emular un sistema de cache real implementado con Redis.

Los m√≥dulos implementados son los siguientes:

## Scraper 
Captura las alertas desde el mapa de Waze (https://www.waze.com/es-419/live-map/) mediante scraper.py
## Almacenamiento
Este m√≥dulo se encarga de el almacenamiento de los datos previamente recuperados usando la base de datos no relacional MongoDB garantizando la persistencia de los datos a pesar de detener los servicios de Docker.
## Generator
M√≥dulo encargado de generar tr√°fico simulado mediante generator.py usando t√©cnicas de distribuci√≥n de datos similares al comportamiento real de consultas. Emula la popularidad de las consultas mediante la funci√≥n de distribuci√≥n Zipf,propiedad estad√≠stica que establece que la frecuencia de un evento es inversamente proporcional a su rango en una lista ordenada por frecuencia. En otras palabras, esto genera que algunos datos sean m√°s consultados que otros(m√°s populares) emulando el comportamiento real de los usuarios.
## Cache
En el backend del cach√© se uso Redis. Este se encarg√≥ de almacenar las consultas m√°s frecuentes previamente simuladas con el m√≥dulo generator. Se estableci√≥ 5MB como ta√±ano de memoria de cache y se us√≥ un TTL de 5 minutos y una pol√≠tica de remoci√≥n LFU.
Esto permiti√≥ una taza de aciertos sobre el 70 % y mantuvo en cache entre 200 a 250 registros durante el tiempo de an√°lisis. Este an√°lisis se realiz√≥ con un script de python para generar gr√°ficos a partir de las m√©tricas obtenidas durante una hora de ejecuci√≥n(graficar_metricas.py). El gr√°fico obtenido fue el siguiente:

![Gr√°fico cache](grafico_cache.png)

*En relaci√≥n al resultado gr√°ficos, cabe mencionar que la tasa de aciertos(linea azul) se ve como una l√≠nea recta debido a que este dato fluct√∫a entre 0 y 1, por lo que a gran escala se ve como si fuera una l√≠nea recta.

## Filtrado
Este m√≥dulo se encarg√≥ del filtrado de los datos provenientes de la base de datos(almacenamiento). Para ello se implement√≥ el c√≥digo filtrado.py que clasific√≥ los registros de alertas por comunas a partir de sus coordenadas(latitud, longitud), tambi√©n elimin√≥ registros repetidos mediante los siguientes criterios:

1. Pertenecieron al mismo tipo de incidente
2. Ocurrieron en un rango temporal de 30 minutos de diferencia
3. Tuvieron una distancia espacial definida en el c√≥digo como eps=0.0005
4. Ocurrieron en la misma comuna

Luego orden√≥ los datos en un archivo formato .csv y lo subi√≥ a Hadoop Distributed File System(HDFS).

## Procesamiento
Mediante el uso de Apache Pig se obtuvo el archivo .csv derivado del m√≥dulo filtrado y se us√≥ para procesar algunas consultas en PigLatin (entrega 2)

## Visualizaci√≥n (entrega 3)
Este m√≥dulo implement√≥ un script llamado visualizaci√≥n.py, el cual se encarg√≥ de extraer el archivo .csv de HDFS para indexarlo en ElasticSearch. Gracias a la herramienta de visualizaci√≥n de datos Kibana se pudo consultar los datos y generar gr√°ficos.

# Persistencia de los datos
Se garantiz√≥ la persistencia de los datos mediante el uso de vol√∫menes en √°reas cr√≠ticas del sistema. 

# Pipelines

Para la ejecuci√≥n del proyecto se implementaron 3 formas de proceder (pipelines).

1. Forma 1 (pipeline 1):
   Enfocado en la estracci√≥n de datos de waze y almacenamiento en base de datos MongoDB.
   Tambien ejecuta el modulo generator y cache, los cuales trabajan con los datos a medida que se van extrayendo en tiempo real.

   Modulos involucrados:
   - Scraper
   - Almacenamiento
   - Generator
   - Cache

2. Forma 2 (pipeline 2):
   Este pipeline filtra y procesa los datos luego de recolectar la cantidad deseadas en almacenamiento usando HDFS y Apache Pig. 

     Modulos involucrados:
   - Almacenamiento
   - Filtrado
   - Procesamiento

3. Forma 3 (pipeline 3):
   Pipeline enfocado en el uso del m√≥dulo visualizaci√≥n. Si ya se cuenta con el archivo .csv con los datos filtrados en HDFS, al ejecutar este pipeline se indexan los datos en ElasticSearch y quedan disponibles para interacci√≥n con Kibana.

   Modulos involucrados:
   - Almacenamiento
   - Visualizaci√≥n
     
5. Forma 4 (pipeline 4):
   Este pipeline es √∫til cuando ya se realiz√≥ todo el proceso anterior y los datos ya est√°n cargados en Kibana. Su uso est√° principalmente orientado a la interacci√≥n con la interfaz visual de Kibana.
   

Para ejecutar un pipeline se debe usar el comando que sigue en la ubicaci√≥n de la carpeta del proyecto.

```bash
chmod +x pipelines/pipeline1.py
./pipelines/pipeline1.py
    
```


# Otros comandos √∫tiles

Para ver las m√©tricas de cache gr√°ficadas:

```bash
python graficar_metricas.py       
```
NOTA: Ejecutar mientras el cache est√° en funcionamiento (pipeline 1).


# Comentarios

Los archivos de gran tama√±o y la configuraci√≥n de hadoop no est√°n presentes en el repositorio
